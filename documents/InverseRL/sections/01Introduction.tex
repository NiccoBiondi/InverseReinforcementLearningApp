\section{Introdution}
Reinforcement Learning (RL) is an area of machine learning concerned with how a particular artificial agent makes several actions inside an environment in order to maximize some cumulative rewards. With this kind of technique, the agent has the possibility to learn the right behavior to adopt in a certain environment through the \textsl{trial-and-error} mechanism. To achieve these goals, a policy network (the agent behavior) is defined and it takes the current environment state to perform the best action in the environment. Nowadays, the RL achieves superhuman performance in a range of environments, but requires that a designer manually specify a reward function.

In the Inverse Reinforcement Learning (IRL), the policy does not receive the reward directly from the environment during the training process. Instead,  we assume that there is a human in the loop who has an intention for the agent’s task, and communicates this intention to the agent using one the preferences feedback channel.  If the policy model mimics the human expert’s behavior well, it can achieve the performance of the human on the task. In this project, we implement the Inverse Reinforcement Learning method described in\ \cite{NIPS2018_8025} to train a policy agent to achieve the goal in the \textsl{Minigrid} environment\ \cite{gym_minigrid}. We use this kind of simple environment because, unlike is described in the original algorithm, we don't use a set of human demonstrations provided by an expert to pretrain the policy. In our case we use only the preferences, the human labels\ \cite{NIPS2018_8025}, provided by the user to define a reward model function which will give rewards for policy training. This choice allows us to train simultaneously the policy and the reward model. 
The human labels are collected thanks to a simple application, where the user can specify his preference between pairs of small segments of agent trajectories.
 
 In video games, the RL is used to define the Non-Player Characters (NPCs), that sometimes have superhuman abilities, overcoming the player skills. On the other hand, the RL can produce a predictable NPCs, making the game experience boring. So, in these cases, the players will not be involved in the game, abandoning it. 
 
 The scope of our project is to propose an IRL tool with which the video game designers can control the NPCs (agents) behaviour in order to create more interactive and engaging video games.
 
