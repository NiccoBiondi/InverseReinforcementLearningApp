\section{IRL Tool}\label{04}

To train the reward model the human has to create an annotation buffer where inside there are all the preferences the human made. A preference is a tuple $( \sigma^1, \sigma^2, \mu)$, where $\sigma^1, \sigma^2$ represent the short trajectory segments of agent's behaviour and $\mu$ defines the human preference between the two clips (that can be $(1, 0)$, $(0, 1)$, $(0, 0)$ and $(0.5, 0.5)$).We create a simple application which include the IRL process described in Section\ \ref{method}, with which the user can easily express his preferences between pairs of clips and change them if they are not correct.

When the application starts, the user sees the main view, that is represented in the left image in Figure\ \ref{fig:Application}.
In this window, he can decide to load a previous session, tapping the "Load Checkpoint" button or starting a new one session, tapping the "Set starting setting" button. Anyway, after clicking the "Start" button, the tool changes window, the right picture of Figure\ \ref{fig:Application}. Here the user can start the IRL process described in Algorithm\ \ref{alg:train_proto} tapping the button "process".
To improve the user experience, we connect each training iteration to the click of this button: the number of clicks corresponds to the number of epochs (M).

We also include the first three steps of the Algorithm\ \ref{alg:train_proto} in the main loop. At epoch $0$ the policy creates the "initial trajectories" used to pretrain the reward model. To speed up this process, the application saves the reward model weights in a folder specifying the adopted environment and the reward model hyper-parameters. In this way, if the user will start new IRL project with same configurations for the environment and the reward model, our application will load these weights, skipping the reward model pretrain.

With the IRL tool, the user also decides to label manually the clips or not. Selecting the oracle check button, the application will give artificial preferences trough an Oracle. Instead the human labeling is supported with the display of the current clips and with the "left", "right", "discard" and "both" buttons. Depending on which button he clicks, we give to this pair of clips the corresponding label (respectively $(1, 0)$, $(0, 1)$, $(0, 0)$, $(0.5, 0.5)$) and we put that in the annotation buffer. 

The user can check his last preferences in the History window, presented in Figure\ \ref{fig:Application}. With this feature, he can look again the clips and decide if the given preferences are correct or not. If not he can select the wrong ones and label them again.

Finally, we consider to include saving methods: auto-save and manual ones. The former is made in the three main step of the Algorithm\ \ref{alg:train_proto}, that is during the policy training, during the annotation and at the end of the reward model training. The latter is achieved when the user click "Save bla bla" in the File menu. In this case, the user can specify the folder where he wants to save the current system state. This is composed by the current policy and reward model weights and by the annotation buffer state (the user preferences). However, in both the saving methods, the tool do not save the system state during the reward model training, but only at the beginning and the end of that. 